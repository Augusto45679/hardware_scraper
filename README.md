# Hardware Scraper ğŸ•·ï¸

Este proyecto es un sistema de web scraping avanzado y escalable desarrollado con **Scrapy** y **Playwright**, diseÃ±ado para extraer informaciÃ³n detallada de productos de hardware de mÃºltiples tiendas de comercio electrÃ³nico en LatinoamÃ©rica.

El sistema no solo extrae datos, sino que tambiÃ©n procesa, limpia, valida y almacena la informaciÃ³n en una base de datos **MongoDB**, gestionando eficientemente las imÃ¡genes de los productos mediante **Cloudinary**.

## ğŸš€ CaracterÃ­sticas Principales

*   **Multi-Spider**: Soporte para mÃºltiples sitios web (`Mercado Libre`, `Compra Gamer`, `Falabella`, `Paris.cl`, `SP Digital`).
*   **Renderizado DinÃ¡mico**: IntegraciÃ³n con **Playwright** para scrapear sitios que dependen fuertemente de JavaScript.
*   **Pipeline Inteligente de ImÃ¡genes**:
    *   Descarga local de imÃ¡genes.
    *   Subida automÃ¡tica a **Cloudinary**.
    *   **DeduplicaciÃ³n inteligente**: Evita resubir imÃ¡genes ya existentes utilizando hashes de contenido.
*   **Procesamiento de Datos**:
    *   **Limpieza**: NormalizaciÃ³n de precios, textos y espacios.
    *   **ValidaciÃ³n**: Asegura que los campos crÃ­ticos (precio, nombre, ID) estÃ©n presentes.
    *   **DeduplicaciÃ³n**: Evita duplicados en la base de datos basÃ¡ndose en IDs Ãºnicos.
*   **Almacenamiento NoSQL**: Persistencia de datos en **MongoDB** con un esquema flexible.
*   **ConfiguraciÃ³n Centralizada**: GestiÃ³n de credenciales y configuraciones mediante variables de entorno (`.env`).

## ğŸ“‹ Requisitos Previos

AsegÃºrate de tener instalado lo siguiente en tu sistema:

*   **Python 3.9+**
*   **MongoDB** (corriendo localmente o una instancia remota)
*   **Cuenta de Cloudinary** (para el almacenamiento de imÃ¡genes en la nube)

## ğŸ› ï¸ InstalaciÃ³n

1.  **Clonar el repositorio:**

    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd hardware_scraper
    ```

2.  **Crear y activar un entorno virtual:**

    *   Windows:
        ```bash
        python -m venv venv
        .\venv\Scripts\activate
        ```
    *   macOS/Linux:
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        ```

3.  **Instalar dependencias:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Instalar navegadores de Playwright:**

    ```bash
    playwright install
    ```

## âš™ï¸ ConfiguraciÃ³n

Crea un archivo `.env` en la raÃ­z del proyecto (puedes usar `.env.example` como base) y configura las siguientes variables:

```ini
# ConfiguraciÃ³n de MongoDB
MONGO_URI=mongodb://localhost:27017
MONGO_DATABASE=hardware_db
MONGO_COLLECTION=products

# Credenciales de Cloudinary
CLOUDINARY_CLOUD_NAME=tu_cloud_name
CLOUDINARY_API_KEY=tu_api_key
CLOUDINARY_API_SECRET=tu_api_secret
```

## ğŸ•·ï¸ Spiders Disponibles

El proyecto cuenta con los siguientes spiders en el directorio `hardwareprices/spiders`:

| Spider | Nombre (para ejecutar) | DescripciÃ³n |
| :--- | :--- | :--- |
| **Mercado Libre** | `mercadolibre` | Busca productos en Mercado Libre (bÃºsqueda general). |
| **Compra Gamer** | `compragamer` | Extrae productos de Compra Gamer (categorÃ­a especÃ­fica). |
| **Falabella** | `falabella` | Scrapea productos de tecnologÃ­a en Falabella. |
| **Paris.cl** | `pariscl` | Extrae datos de Paris.cl. |
| **SP Digital** | `spdigital` | Scrapea el catÃ¡logo de SP Digital. |

## â–¶ï¸ Uso

Para ejecutar un spider y guardar los resultados, utiliza el comando `scrapy crawl`.

**Ejemplo bÃ¡sico:**

```bash
scrapy crawl mercadolibre
```

**Guardar salida en JSON:**

```bash
scrapy crawl compragamer -O output.json
```

**Pasar argumentos (si el spider lo soporta):**

```bash
scrapy crawl mercadolibre -a search="rtx 3060"
```

## ğŸ“‚ Estructura del Proyecto

```text
hardware_scraper/
â”œâ”€â”€ hardwareprices/
â”‚   â”œâ”€â”€ spiders/           # DefiniciÃ³n de los spiders
â”‚   â”œâ”€â”€ pipelines/         # LÃ³gica de procesamiento (imÃ¡genes, mongo, limpieza)
â”‚   â”œâ”€â”€ items.py           # DefiniciÃ³n del modelo de datos
â”‚   â”œâ”€â”€ settings.py        # ConfiguraciÃ³n de Scrapy
â”‚   â””â”€â”€ middlewares.py     # Middlewares personalizados
â”œâ”€â”€ scraped_images/        # Directorio de descarga local de imÃ¡genes
â”œâ”€â”€ scrapy.cfg             # Archivo de configuraciÃ³n de despliegue
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â””â”€â”€ .env                   # Variables de entorno (no commitear)
```

## ğŸ“Š Modelo de Datos

Cada producto extraÃ­do se almacena con la siguiente estructura (definida en `items.py`):

*   `product_id`: Hash Ãºnico del producto.
*   `store_id`: Identificador de la tienda.
*   `product_name`: TÃ­tulo del producto.
*   `price_original`: Precio de lista.
*   `price_current`: Precio actual.
*   `image_url`: URL pÃºblica de la imagen (Cloudinary).
*   `specs_normalized`: Especificaciones tÃ©cnicas procesadas.
*   `scraped_at`: Fecha y hora de la extracciÃ³n.

## ğŸ“ Pipelines Activos

El orden de procesamiento en `settings.py` es:

1.  `CleaningPipeline`: Limpia espacios y normaliza textos.
2.  `ValidationPipeline`: Descarta items sin precio o nombre.
3.  `DeduplicationPipeline`: Evita procesar el mismo producto dos veces en la misma ejecuciÃ³n.
4.  `CustomImagesPipeline`: Descarga imÃ¡genes localmente.
5.  `SmartCloudinaryPipeline`: Sube imÃ¡genes a Cloudinary (si no existen ya).
6.  `MongoPipeline`: Guarda el item final en MongoDB.